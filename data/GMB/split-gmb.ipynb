{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year: year\n",
      "2002       1\n",
      "2004     651\n",
      "2005    3300\n",
      "2006    1888\n",
      "2007    1081\n",
      "2008    1033\n",
      "2009     501\n",
      "2010     677\n",
      "2011      33\n",
      "dtype: int64\n",
      "source: source\n",
      "http    9165\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('/home/weisi/Temporal/data/GMB/gmb-2.2.0-seqtag.json', lines=True)\n",
    "df['year'] =  pd.to_datetime(df['time']).dt.year\n",
    "print('year:',df.groupby('year').size())\n",
    "print('source:',df.groupby('source').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tim-nam', 'org-leg', 'tim-dow', 'art-nam', 'per-fam', 'per-giv', 'org-nam', 'tim-clo', 'gpe-nam', 'art-add', 'per-ini', 'nat-nam', 'per-nam', 'geo-nam', 'eve-nam', 'tim-yoc', 'eve-ord', 'tim-dat', 'per-tit', 'per-mid', 'tim-dom', 'tim-moy', 'per-ord', 'O'}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set()\n",
    "for labels in df['ner_tags']:\n",
    "    unique_labels.update(labels)\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre: genre\n",
      "newspaper    9165\n",
      "dtype: int64\n",
      "subcorpus: subcorpus\n",
      "Voice of America    9165\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('genre:',df.groupby('genre').size())\n",
    "print('subcorpus:',df.groupby('subcorpus').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = '/home/weisi/Temporal/data/GMB'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    \n",
    "for year in range(2005, 2009):\n",
    "    df_year = df[df['year'] == year]   \n",
    "    #filename: 'gmb_2006.json'...\n",
    "    filename = f'gmb_{year}.json'\n",
    "    # save to json file\n",
    "    df_year.to_json(os.path.join(folder_path, filename), orient='records') \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "folder_path = '/home/weisi/Temporal/data/GMB'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# devide dataset to 3 time periods\n",
    "df_before_2005 = df[df['year'] < 2006]\n",
    "df_2006_2007 = df[df['year'].isin([2006, 2007])]\n",
    "df_after_2008 = df[df['year'] > 2007]\n",
    "\n",
    "# reduce the datasets to the same size\n",
    "min_size = min(len(df_before_2005), len(df_2006_2007), len(df_after_2008))\n",
    "\n",
    "\n",
    "df_before_2005_sampled = df_before_2005.sample(n=min_size, random_state=1)\n",
    "df_2006_2007_sampled = df_2006_2007.sample(n=min_size, random_state=1)\n",
    "df_after_2008_sampled = df_after_2008.sample(n=min_size, random_state=1)\n",
    "\n",
    "def save_datasets(df, period):\n",
    "    # split train, validation and test datasets by ratio 0.7 0.15 0.15\n",
    "    train, test = train_test_split(df, test_size=0.3, random_state=1)  \n",
    "    validation, test = train_test_split(test, test_size=0.5, random_state=1)  \n",
    "\n",
    "    # save files\n",
    "    train_filename = f'gmb_{period}_train.json'\n",
    "    validation_filename = f'gmb_{period}_validation.json'\n",
    "    test_filename = f'gmb_{period}_test.json'\n",
    "    train.to_json(os.path.join(folder_path, train_filename), orient='records', lines=True)\n",
    "    validation.to_json(os.path.join(folder_path, validation_filename), orient='records', lines=True)\n",
    "    test.to_json(os.path.join(folder_path, test_filename), orient='records', lines=True)\n",
    "\n",
    "\n",
    "save_datasets(df_before_2005_sampled, 'T1_2004-2005')\n",
    "save_datasets(df_2006_2007_sampled, 'T2_2006-2007')\n",
    "save_datasets(df_after_2008_sampled, 'T3_2008-2011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "folder_path = '/home/weisi/TemporalAssessment/data/GMB'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    \n",
    "\n",
    "def split_and_save_datasets(df,period,seed,folder_path):\n",
    "    # split train, validation and test datasets by ratio 0.6 0.2 0.2\n",
    "    train, rest = train_test_split(df, test_size=0.4, random_state=seed)  \n",
    "    validation, test = train_test_split(rest, test_size=0.5, random_state=seed)  \n",
    "    # save files\n",
    "    train_filename = f'{period}-train.json'\n",
    "    validation_filename = f'{period}-validation.json'\n",
    "    test_filename = f'{period}-test.json'\n",
    "    train.to_json(os.path.join(folder_path, train_filename), orient='records', lines=True)\n",
    "    validation.to_json(os.path.join(folder_path, validation_filename), orient='records', lines=True)\n",
    "    test.to_json(os.path.join(folder_path, test_filename), orient='records', lines=True)\n",
    "\n",
    "# split the factoid questions data\n",
    "\n",
    "t1 = df[df['year'].isin([2004, 2005])]\n",
    "t2 = df[df['year'].isin([2006, 2007])]\n",
    "t3 = df[df['year'].isin([2008, 2009])]\n",
    "t4 = df[df['year'].isin([2010, 2011])]\n",
    "df_all_year= df[df['year'].isin([2004, 2009])]\n",
    "\n",
    "min_size = min(len(t1), len(t2),len(t3), len(t4))\n",
    "\n",
    "for seed in range(1, 6):  # randomly split 5 times\n",
    "    folder_path ='/home/weisi/TemporalAssessment/data/GMB/seed{}/'.format(seed)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    t1_sampled = t1.sample(n=min_size, random_state=seed)\n",
    "    t2_sampled = t2.sample(n=min_size, random_state=seed)\n",
    "    t3_sampled = t3.sample(n=min_size, random_state=seed)\n",
    "    t4_sampled = t4.sample(n=min_size, random_state=seed)\n",
    "    all_year_sampled = df_all_year.sample(n=min_size, random_state=seed)\n",
    "    split_and_save_datasets(t1_sampled, 'gmb-T1_2004_2005',seed,folder_path)\n",
    "    split_and_save_datasets(t2_sampled, 'gmb-T2_2006_2007',seed,folder_path)\n",
    "    split_and_save_datasets(t3_sampled, 'gmb-T3_2008_2009',seed,folder_path)\n",
    "    split_and_save_datasets(t4_sampled, 'gmb-T4_2010_2011',seed,folder_path)\n",
    "    split_and_save_datasets(all_year_sampled, 'gmb-AY_2004_2009',seed,folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

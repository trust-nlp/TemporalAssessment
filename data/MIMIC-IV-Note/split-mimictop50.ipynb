{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('/home/weisi/TemporalAssessment/data/MIMIC-IV-Note/mimic_final.json', 'r', encoding='utf-8') as f:\n",
    "    df=pd.read_json(f,lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "2008 - 2010    149591\n",
      "2011 - 2013     77556\n",
      "2014 - 2016     61476\n",
      "2017 - 2019     43022\n",
      "2020 - 2022         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby('time').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28926\n",
      "time\n",
      "2008 - 2010    137631\n",
      "2011 - 2013     70089\n",
      "2014 - 2016     56411\n",
      "2017 - 2019     38588\n",
      "2020 - 2022         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "empty_label_count = df['label'].apply(lambda x: len(x) == 0).sum()\n",
    "print(empty_label_count)\n",
    "filtered_df = df[df['label'].apply(lambda x: len(x) > 0)]\n",
    "print(filtered_df.groupby('time').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319037/2198230877.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['token_length'] = filtered_df.apply(compute_token_length, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"length_distribution = filtered_df['token_length'].value_counts().sort_index()\\nprint(length_distribution)\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def compute_token_length(row):\n",
    "    tokens = tokenizer.tokenize(row['text'])\n",
    "    return len(tokens)\n",
    "\n",
    "filtered_df['token_length'] = filtered_df.apply(compute_token_length, axis=1)\n",
    "'''length_distribution = filtered_df['token_length'].value_counts().sort_index()\n",
    "print(length_distribution)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token length statistics:\n",
      "count    302720.000000\n",
      "mean       1979.489783\n",
      "std         835.777183\n",
      "min          13.000000\n",
      "25%        1391.000000\n",
      "50%        1856.000000\n",
      "75%        2418.000000\n",
      "max       10986.000000\n",
      "Name: token_length, dtype: float64\n",
      "Number of entries with token length below 256 by time:\n",
      "time\n",
      "2008 - 2010    91\n",
      "2011 - 2013    26\n",
      "2014 - 2016    28\n",
      "2017 - 2019    28\n",
      "2020 - 2022     0\n",
      "dtype: int64\n",
      "\n",
      "Number of entries with token length below 512 by time:\n",
      "time\n",
      "2008 - 2010    816\n",
      "2011 - 2013    277\n",
      "2014 - 2016    206\n",
      "2017 - 2019    186\n",
      "2020 - 2022      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319037/3065315887.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  counts_below_256 = filtered_df.groupby('time').apply(count_below_threshold, threshold=256)\n",
      "/tmp/ipykernel_319037/3065315887.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  counts_below_512 = filtered_df.groupby('time').apply(count_below_threshold, threshold=512)\n"
     ]
    }
   ],
   "source": [
    "length_statistics = filtered_df['token_length'].describe()\n",
    "print(\"Token length statistics:\")\n",
    "print(length_statistics)\n",
    "\n",
    "def count_below_threshold(group, threshold):\n",
    "    return (group['token_length'] < threshold).sum()\n",
    "\n",
    "counts_below_256 = filtered_df.groupby('time').apply(count_below_threshold, threshold=256)\n",
    "counts_below_512 = filtered_df.groupby('time').apply(count_below_threshold, threshold=512)\n",
    "print(\"Number of entries with token length below 256 by time:\")\n",
    "print(counts_below_256)\n",
    "print(\"\\nNumber of entries with token length below 512 by time:\")\n",
    "print(counts_below_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df=filtered_df[filtered_df['token_length'] > 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                count         mean         std     min     25%     50%  \\\n",
      "time                                                                     \n",
      "2008 - 2010  137540.0  1922.443711  800.192187   257.0  1359.0  1812.0   \n",
      "2011 - 2013   70062.0  1979.130213  830.837909   261.0  1392.0  1858.0   \n",
      "2014 - 2016   56383.0  2025.577373  856.778981   261.0  1418.0  1894.0   \n",
      "2017 - 2019   38560.0  2124.324300  906.687133   260.0  1481.0  1975.0   \n",
      "2020 - 2022       1.0  2701.000000         NaN  2701.0  2701.0  2701.0   \n",
      "\n",
      "                 75%      max  \n",
      "time                           \n",
      "2008 - 2010  2346.25  10970.0  \n",
      "2011 - 2013  2421.00  10986.0  \n",
      "2014 - 2016  2473.50  10273.0  \n",
      "2017 - 2019  2598.25   9186.0  \n",
      "2020 - 2022  2701.00   2701.0  \n"
     ]
    }
   ],
   "source": [
    "grouped = filtered_df.groupby('time')['token_length'].describe()\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label list:  0      I2510\n",
      "1      I5032\n",
      "2       I509\n",
      "3      I4891\n",
      "4       J449\n",
      "5       K219\n",
      "6       I252\n",
      "7      Z9861\n",
      "8      G8929\n",
      "9       I129\n",
      "10      N189\n",
      "11      M109\n",
      "12      E785\n",
      "13     M1990\n",
      "14    Z86718\n",
      "15     Z7982\n",
      "16     Z7901\n",
      "17      Z794\n",
      "18      F419\n",
      "19      N179\n",
      "20       I10\n",
      "21      F329\n",
      "22       D62\n",
      "23      D696\n",
      "24    Z87891\n",
      "25      D649\n",
      "26     Z8673\n",
      "27      E875\n",
      "28    F17200\n",
      "29      J189\n",
      "30      E871\n",
      "31      E039\n",
      "32      N400\n",
      "33      E119\n",
      "34       Z66\n",
      "35      N183\n",
      "36      M810\n",
      "37      Z951\n",
      "38      E860\n",
      "39     G4733\n",
      "40    J45998\n",
      "41     E7800\n",
      "42      D509\n",
      "43      Y929\n",
      "44      E669\n",
      "45      N390\n",
      "46      N186\n",
      "47     Z7902\n",
      "48      E872\n",
      "49     K5900\n",
      "dtype: object\n",
      "label list:  0      Z7901\n",
      "1       E785\n",
      "2       M810\n",
      "3     Z87891\n",
      "4      E7800\n",
      "5       E119\n",
      "6       I129\n",
      "7       N189\n",
      "8      G4733\n",
      "9       I509\n",
      "10      D649\n",
      "11     I2510\n",
      "12       I10\n",
      "13      E875\n",
      "14    J45998\n",
      "15      K219\n",
      "16      N390\n",
      "17      N179\n",
      "18      N183\n",
      "19     K5900\n",
      "20     Z8673\n",
      "21      N186\n",
      "22     I5032\n",
      "23       D62\n",
      "24      F329\n",
      "25       Z66\n",
      "26      E669\n",
      "27      E039\n",
      "28     G8929\n",
      "29      F419\n",
      "30     I4891\n",
      "31      M109\n",
      "32      Y929\n",
      "33      I252\n",
      "34      E872\n",
      "35     Z7902\n",
      "36      J189\n",
      "37      Z951\n",
      "38      Z794\n",
      "39      J449\n",
      "40     M1990\n",
      "41      D509\n",
      "42     Z9861\n",
      "43      N400\n",
      "44    F17200\n",
      "45      E871\n",
      "46      E860\n",
      "47    Z86718\n",
      "48     Z7982\n",
      "49      D696\n",
      "dtype: object\n",
      "label list:  0       E860\n",
      "1        I10\n",
      "2       E119\n",
      "3       F419\n",
      "4       E669\n",
      "5       E785\n",
      "6     Z87891\n",
      "7        D62\n",
      "8      I4891\n",
      "9       E039\n",
      "10     M1990\n",
      "11    Z86718\n",
      "12     Z7901\n",
      "13      F329\n",
      "14      J189\n",
      "15      N179\n",
      "16      E872\n",
      "17      I129\n",
      "18       Z66\n",
      "19      M810\n",
      "20      N400\n",
      "21      N390\n",
      "22     I2510\n",
      "23      K219\n",
      "24      E871\n",
      "25     Z7982\n",
      "26      I509\n",
      "27      D509\n",
      "28      J449\n",
      "29    J45998\n",
      "30    F17200\n",
      "31     G4733\n",
      "32     G8929\n",
      "33      Z951\n",
      "34      N183\n",
      "35     Z8673\n",
      "36      I252\n",
      "37      D696\n",
      "38      E875\n",
      "39     Z7902\n",
      "40      Z794\n",
      "41      M109\n",
      "42      D649\n",
      "43     E7800\n",
      "44     Z9861\n",
      "45      N189\n",
      "46      N186\n",
      "47      Y929\n",
      "48     K5900\n",
      "49     I5032\n",
      "dtype: object\n",
      "label list:  0        I10\n",
      "1       E669\n",
      "2       E785\n",
      "3       N179\n",
      "4       E039\n",
      "5        Z66\n",
      "6       N390\n",
      "7      Z7902\n",
      "8      Z8673\n",
      "9       F419\n",
      "10      I509\n",
      "11     I4891\n",
      "12    Z87891\n",
      "13     G4733\n",
      "14      D649\n",
      "15      J449\n",
      "16     I2510\n",
      "17    Z86718\n",
      "18      Y929\n",
      "19       D62\n",
      "20      E871\n",
      "21      K219\n",
      "22      E119\n",
      "23      Z794\n",
      "24     Z7901\n",
      "25      D696\n",
      "26      E860\n",
      "27      F329\n",
      "28     K5900\n",
      "29     E7800\n",
      "30      I129\n",
      "31      E875\n",
      "32     M1990\n",
      "33      N400\n",
      "34      M109\n",
      "35      N183\n",
      "36      E872\n",
      "37      Z951\n",
      "38      I252\n",
      "39      N189\n",
      "40      J189\n",
      "41     I5032\n",
      "42      D509\n",
      "43      N186\n",
      "44     Z7982\n",
      "45    F17200\n",
      "46      M810\n",
      "47     G8929\n",
      "48     Z9861\n",
      "49    J45998\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed=1\n",
    "\n",
    "folder_path ='/home/weisi/TemporalAssessment/data/MIMIC-IV-Note/seed{}/'.format(seed)\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "ndf=filtered_df[['uid', 'did', 'time', 'text', 'label']]\n",
    "# devide dataset to 4 time periods\n",
    "df_2008_2010 = ndf[ndf['time'] == '2008 - 2010']\n",
    "df_2011_2013 = ndf[ndf['time'] == '2011 - 2013']\n",
    "df_2014_2016 = ndf[ndf['time'] == '2014 - 2016']\n",
    "df_2017_2019 = ndf[ndf['time'] == '2017 - 2019']\n",
    "\n",
    "# reduce the datasets to the same size\n",
    "min_size = min(len(df_2008_2010), len(df_2011_2013), len(df_2014_2016), len(df_2017_2019))\n",
    "\n",
    "\n",
    "df_2008_2010_sampled = df_2008_2010.sample(n=min_size, random_state=seed)\n",
    "df_2011_2013_sampled = df_2011_2013.sample(n=min_size, random_state=seed)\n",
    "df_2014_2016_sampled = df_2014_2016.sample(n=min_size, random_state=seed)\n",
    "df_2017_2019_sampled = df_2017_2019.sample(n=min_size, random_state=seed)\n",
    "\n",
    "\n",
    "def save_datasets(df, period,seed):\n",
    "    print('label number: ',len(pd.Series(df['label'].explode().unique())))\n",
    "    # split train, validation and test datasets by ratio 0.6 0.2 0.2\n",
    "    train, test = train_test_split(df, test_size=0.4, random_state=seed)  \n",
    "    validation, test = train_test_split(test, test_size=0.5, random_state=seed)  \n",
    "\n",
    "    # save files\n",
    "    train_filename = f'{period}_train.json'\n",
    "    validation_filename = f'{period}_validation.json'\n",
    "    test_filename = f'{period}_test.json'\n",
    "    train.to_json(os.path.join(folder_path, train_filename), orient='records', lines=True)\n",
    "    validation.to_json(os.path.join(folder_path, validation_filename), orient='records', lines=True)\n",
    "    test.to_json(os.path.join(folder_path, test_filename), orient='records', lines=True)\n",
    "\n",
    "\n",
    "save_datasets(df_2008_2010_sampled, 'T1',seed)\n",
    "save_datasets(df_2011_2013_sampled, 'T2',seed)\n",
    "save_datasets(df_2014_2016_sampled, 'T3',seed)\n",
    "save_datasets(df_2017_2019_sampled, 'T4',seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_period = min_size // 4  # floor\n",
    "\n",
    "# create a all year data that draw equal data from 4 time periods and has same size as other time periods\n",
    "allyear_sampled = pd.concat([\n",
    "    df_2008_2010.sample(n=samples_per_period, random_state=1),\n",
    "    df_2011_2013.sample(n=samples_per_period, random_state=1),\n",
    "    df_2014_2016.sample(n=samples_per_period, random_state=1),\n",
    "    df_2017_2019.sample(n=samples_per_period, random_state=1)\n",
    "])\n",
    "# randomlize the order od all year data\n",
    "allyear_sampled = allyear_sampled.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "save_datasets(allyear_sampled, 'Allyear_sampled')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

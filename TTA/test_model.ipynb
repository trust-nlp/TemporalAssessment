{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config_classes import ModelArguments, DataTrainingArguments\n",
    "from transformers.modeling_outputs import (SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,)\n",
    "from transformers.modeling_roberta import *\n",
    "\n",
    "from transformers import RobertaModel, RobertaConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import (\n",
    "    SequenceClassifierOutput, \n",
    "    TokenClassifierOutput, \n",
    "    QuestionAnsweringModelOutput\n",
    ")\n",
    "import transformers\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from typing import List, Optional\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(config.hidden_size, config.hidden_size) # (in_feature, out_feature) didn't change here\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.up = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.LayerNorm(x + self.dropout(self.up(self.act(self.down(x)))))\n",
    "\n",
    "class AdaptedRobertaForSequenceClassification(RobertaModel):\n",
    "    \"\"\"\n",
    "    Custom Roberta model for handling multiple tasks: classification, NER, QA.\n",
    "    Inherits from RobertaModel and adds task-specific heads and an adapter.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.classifier = RobertaClassificationHead(config) \n",
    "        \n",
    "        #self.adapter = Adapter(config)  #this is only one adapter.\n",
    "        # Add adapters. 2 adapters\n",
    "        self.adapter1 = nn.ModuleList([Adapter(config) for _ in range(config.num_hidden_layers // 2)])\n",
    "        self.adapter2 = nn.ModuleList([Adapter(config) for _ in range(config.num_hidden_layers // 2)])\n",
    "\n",
    "        # Add side block output head.\n",
    "        self.adapter1_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.adapter2_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):#-> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        # Get the outputs of each layer\n",
    "        '''outputs = super().forward(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            output_hidden_states=True\n",
    "        )'''\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        ) # self.roberta is a RobertaModel class, the output is class BaseModelOutputWithPoolingAndCrossAttentions\n",
    "        #contains:last_hidden_state;pooler_output;hidden_states;past_key_values;attentions;cross_attentions\n",
    "\n",
    "        sequence_output = outputs[0] # last_hidden_state, the sequence output\n",
    "        if config.task_name == \"ner\":\n",
    "            sequence_output = self.dropout(sequence_output)\n",
    "        #1logits = self.classifier(sequence_output)\n",
    "        layer_outputs = outputs[2] # hidden_states\n",
    "\n",
    "        adapter1_states = layer_outputs[0]\n",
    "        adapter2_states = layer_outputs[0]\n",
    "        # Side block forward propagation.\n",
    "        for i in range(1, len(layer_outputs), 2):\n",
    "            adapter1_states = self.adapter1[i // 2](adapter1_states + layer_outputs[i] + layer_outputs[i + 1])\n",
    "\n",
    "        for i in range(0, len(layer_outputs)-1, 2):\n",
    "            adapter2_states = self.adapter1[i // 2](adapter2_states + layer_outputs[i] + layer_outputs[i + 1])\n",
    "   \n",
    "        pooled_output = adapted_sequence[:, 0, :]  # [CLS] token for classification tasks\n",
    "        logits = self.classification_head(pooled_output)\n",
    "        return SequenceClassifierOutput(logits=logits)\n",
    "        \n",
    "\n",
    "class AdaptedRobertaForTokenClassification(RobertaModel):\n",
    "    \"\"\"\n",
    "    Custom Roberta model for handling multiple tasks: classification, NER, QA.\n",
    "    Inherits from RobertaModel and adds task-specific heads and an adapter.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # Task-specific heads\n",
    "\n",
    "        ner_classifier_dropout = (config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob)\n",
    "        self.dropout = nn.Dropout(ner_classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        #self.qa_head = nn.Linear(config.hidden_size, config.num_labels)  #this cannot be used for generative QA(need to use model like T5, bloom)\n",
    "        # For start and end logits in QA config.num_labels=2ï¼Œalso can be self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        #self.adapter = Adapter(config)  #this is only one adapter.\n",
    "        # Add adapters. 2 adapters\n",
    "        self.adapter1 = nn.ModuleList([Adapter(config) for _ in range(config.num_hidden_layers // 2)])\n",
    "        self.adapter2 = nn.ModuleList([Adapter(config) for _ in range(config.num_hidden_layers // 2)])\n",
    "\n",
    "        # Add side block output head.\n",
    "        self.adapter1_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.adapter2_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):#-> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        # Get the outputs of each layer\n",
    "        '''outputs = super().forward(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            output_hidden_states=True\n",
    "        )'''\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        ) # self.roberta is a RobertaModel class, the output is class BaseModelOutputWithPoolingAndCrossAttentions\n",
    "        #contains:last_hidden_state;pooler_output;hidden_states;past_key_values;attentions;cross_attentions\n",
    "\n",
    "        sequence_output = outputs[0] # last_hidden_state, the sequence output\n",
    "        if config.task_name == \"ner\":\n",
    "            sequence_output = self.dropout(sequence_output)\n",
    "        #1logits = self.classifier(sequence_output)\n",
    "        layer_outputs = outputs[2] # hidden_states\n",
    "\n",
    "        adapter1_states = layer_outputs[0]\n",
    "        adapter2_states = layer_outputs[0]\n",
    "        # Side block forward propagation.\n",
    "        for i in range(1, len(layer_outputs), 2):\n",
    "            adapter1_states = self.adapter1[i // 2](adapter1_states + layer_outputs[i] + layer_outputs[i + 1])\n",
    "\n",
    "        for i in range(0, len(layer_outputs)-1, 2):\n",
    "            adapter2_states = self.adapter1[i // 2](adapter2_states + layer_outputs[i] + layer_outputs[i + 1])\n",
    "\n",
    "        logits = self.ner_head(adapted_sequence)\n",
    "        return TokenClassifierOutput(logits=logits)\n",
    "      \n",
    "# Example usage\n",
    "config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "config.num_labels = 2  # For binary classification\n",
    "config.num_labels_ner = 10  # For NER with 10 different entities\n",
    "model = CustomRobertaForMultipleTasks(config)\n",
    "\n",
    "# Example inputs\n",
    "input_ids = torch.tensor([[0, 1, 2, 3, 4]])  # Example input IDs\n",
    "attention_mask = torch.tensor([[1, 1, 1, 1, 1]])  # Example attention mask\n",
    "\n",
    "# Get outputs for a specific task\n",
    "classification_output = model(input_ids, attention_mask, task=\"classification\")\n",
    "ner_output = model(input_ids, attention_mask, task=\"ner\")\n",
    "qa_output = model(input_ids, attention_mask, task=\"qa\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
